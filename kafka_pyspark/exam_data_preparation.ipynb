{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefc9735-6789-46fd-9e0c-108f2b271cfa",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ea7c6c-c81d-4cad-a898-ff0a7ffb3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a472d2-e145-4cd0-b84e-5c15ed9764db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[1]\")\n",
    "    .appName(\"Exam data preparation\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007450b-4f65-46d9-b300-fc9937e2f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b69f66",
   "metadata": {},
   "source": [
    "### Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda58200-409b-43c4-afbf-b71c332dab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides_schema = (\n",
    "    T.StructType()\n",
    "    .add(\"id\", T.StringType())\n",
    "    .add(\"date\", T.StringType())\n",
    "    .add(\"time\", T.StringType())\n",
    "    .add(\"country_code\", T.StringType())\n",
    "    .add(\"state/province\", T.StringType())\n",
    "    .add(\"population\", T.IntegerType())\n",
    "    .add(\"city/town\", T.StringType())\n",
    "    .add(\"distance\", T.FloatType())\n",
    "    .add(\"latitude\", T.FloatType())\n",
    "    .add(\"longitude\", T.FloatType())\n",
    "    .add(\"landslide_size\", T.StringType())\n",
    "    .add(\"injuries\", T.FloatType())\n",
    "    .add(\"fatalities\", T.FloatType())\n",
    ")\n",
    "\n",
    "df_source_batch_landslides = spark.read.parquet(\n",
    "    \"./data/landslides.parquet\", schema=landslides_schema\n",
    ")\n",
    "df_source_batch_landslides = df_source_batch_landslides.withColumn(\n",
    "    \"value\",\n",
    "    F.to_json(F.struct(*df_source_batch_landslides.columns)).cast(\n",
    "        T.StringType()\n",
    "    ),\n",
    ")\n",
    "dataframe_source_batch_writer_landslides = (\n",
    "    df_source_batch_landslides.select(\"value\")\n",
    "    .write.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"temp_topic\")\n",
    ")\n",
    "\n",
    "print(df_source_batch_landslides.select(\"value\").count())\n",
    "\n",
    "# Run twice to have duplicates to drop\n",
    "dataframe_source_batch_writer_landslides.save()\n",
    "dataframe_source_batch_writer_landslides.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34adda3-6a29-470f-b54c-9048e6d5f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_schema = (\n",
    "    T.StructType()\n",
    "    .add(\"country_code\", T.StringType())\n",
    "    .add(\"country_name\", T.StringType())\n",
    ")\n",
    "\n",
    "df_source_batch_countries = spark.read.csv(\n",
    "    \"./data/countries.csv\", schema=countries_schema\n",
    ")\n",
    "df_source_batch_countries = df_source_batch_countries.withColumn(\n",
    "    \"value\",\n",
    "    F.to_json(F.struct(*df_source_batch_countries.columns)).cast(\n",
    "        T.StringType()\n",
    "    ),\n",
    ")\n",
    "dataframe_source_batch_writer_countries = (\n",
    "    df_source_batch_countries.select(\"value\")\n",
    "    .write.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"countries_topic\")\n",
    ")\n",
    "\n",
    "print(df_source_batch_countries.select(\"value\").count())\n",
    "\n",
    "dataframe_source_batch_writer_countries.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67114a-2246-40b3-952f-7b8c1812d34d",
   "metadata": {},
   "source": [
    "### Prepare landslides data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a29ab9-7455-42c7-a157-8ee9ca5286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"temp_topic\")\n",
    "    .option(\"failOnDataLoss\", \"true\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"message_content\",\n",
    "    F.from_json(F.col(\"value\").cast(\"string\"), landslides_schema),\n",
    ")\n",
    "df_minimal = df.select(\"message_content.*\")\n",
    "\n",
    "location_columns = [\n",
    "    \"country_code\",\n",
    "    \"state/province\",\n",
    "    \"city/town\",\n",
    "    \"population\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "time_columns = [\"date\", \"time\"]\n",
    "df_to_kafka = df_minimal\n",
    "df_to_kafka = df_to_kafka.withColumn(\n",
    "    \"location_columns\", F.struct(location_columns)\n",
    ")\n",
    "df_to_kafka = df_to_kafka.withColumn(\"time_columns\", F.struct(time_columns))\n",
    "df_to_kafka = df_to_kafka.select(\n",
    "    \"id\",\n",
    "    \"distance\",\n",
    "    \"landslide_size\",\n",
    "    \"injuries\",\n",
    "    \"fatalities\",\n",
    "    \"location_columns\",\n",
    "    \"time_columns\",\n",
    ")\n",
    "df_to_kafka = df_to_kafka.withColumn(\n",
    "    \"data_packed_for_kafka\", F.to_json(F.struct(*df_to_kafka.columns))\n",
    ")\n",
    "df_to_kafka.printSchema()\n",
    "query = (\n",
    "    df_to_kafka.select(F.col(\"data_packed_for_kafka\").alias(\"value\"))\n",
    "    .write.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"landslides_topic\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015044a",
   "metadata": {},
   "source": [
    "Check for json correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24f15f-3dc6-4044-8ffe-5fe6cc8dc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meant_to_be_json = df_to_kafka.select(F.col(\"data_packed_for_kafka\")).tail(1)[\n",
    "    0\n",
    "][\"data_packed_for_kafka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c79d2-7728-4b4d-8e4f-1da8a31789f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# prove data is correctly formatted JSON\n",
    "json.loads(meant_to_be_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeb5b4-a490-48d3-b5d5-e54ed13eb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
